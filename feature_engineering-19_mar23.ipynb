{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9eda0e",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application\n",
    "\n",
    "Min-Max scaling, also known as normalization, is a common data preprocessing technique used to rescale numerical features to a specific range. It transforms the original feature values to a new range, typically between 0 and 1, by subtracting the minimum value and dividing by the range (i.e., the difference between the maximum and minimum values).\n",
    "\n",
    "The formula for Min-Max scaling is as follows:\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "\n",
    "where \\( X_{\\text{scaled}} \\) is the scaled value, \\( X \\) is the original value of the feature, \\( X_{\\text{min}} \\) is the minimum value of the feature, and \\( X_{\\text{max}} \\) is the maximum value of the feature.\n",
    "\n",
    "Min-Max scaling is useful in data preprocessing for several reasons:\n",
    "\n",
    "1. **Normalization:** Min-Max scaling ensures that all feature values fall within the same range, making it easier to compare and interpret them. By bringing the values between 0 and 1, it normalizes the data, preventing features with larger magnitudes from dominating the analysis.\n",
    "\n",
    "2. **Feature Comparability:** Scaling the features to a common range helps in comparing their magnitudes. It eliminates the issues arising from different scales and units, allowing for a fair comparison and analysis across features.\n",
    "\n",
    "3. **Algorithm Performance:** Some machine learning algorithms, such as those based on distance calculations (e.g., K-nearest neighbors, clustering algorithms), are sensitive to the scale of features. Min-Max scaling helps these algorithms to work better by bringing features to a similar scale, preventing one feature from dominating the distance calculations.\n",
    "\n",
    "4. **Convergence Speed:** Gradient-based optimization algorithms, like those used in neural networks, converge faster when the features are on a similar scale. Min-Max scaling helps in speeding up the convergence process.\n",
    "\n",
    "Here's an example to illustrate the application of Min-Max scaling:\n",
    "\n",
    "Consider a dataset with a feature \"Age\" representing the age of individuals. The original ages range from 20 to 60 years. To apply Min-Max scaling, we calculate the minimum and maximum values of the feature:\n",
    "\\( X_{\\text{min}} = 20 \\) and \\( X_{\\text{max}} = 60 \\).\n",
    "\n",
    "Suppose we have an individual with an age of 35. Using the Min-Max scaling formula, we can calculate the scaled value:\n",
    "\\[ X_{\\text{scaled}} = \\frac{35 - 20}{60 - 20} = \\frac{15}{40} = 0.375 \\]\n",
    "\n",
    "So, the age of 35 is scaled to 0.375 using Min-Max scaling. Similarly, all other ages in the dataset will be transformed to values between 0 and 1 based on their original values and the range of the feature.\n",
    "\n",
    "Applying Min-Max scaling ensures that the age values are normalized and fall within the range of 0 to 1, making them comparable and suitable for further analysis or modeling tasks.\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8babcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original ages:\n",
      " [[20]\n",
      " [35]\n",
      " [45]\n",
      " [60]]\n",
      "\n",
      "Scaled ages:\n",
      " [[0.   ]\n",
      " [0.375]\n",
      " [0.625]\n",
      " [1.   ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Original data\n",
    "ages = np.array([20, 35, 45, 60]).reshape(-1, 1)\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using the scaler\n",
    "scaled_ages = scaler.fit_transform(ages)\n",
    "\n",
    "# Print the original and scaled values\n",
    "print(\"Original ages:\\n\", ages)\n",
    "print(\"\\nScaled ages:\\n\", scaled_ages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec10181",
   "metadata": {},
   "source": [
    "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as vector normalization or feature scaling by dividing by the L2 norm, is a method used to scale features by dividing each feature vector by its magnitude. It transforms the feature vectors to have a unit length or a Euclidean norm of 1.\n",
    "\n",
    "The formula for Unit Vector scaling is as follows:\n",
    "\\[ X_{\\text{unit}} = \\frac{X}{\\|X\\|_2} \\]\n",
    "\n",
    "where \\( X_{\\text{unit}} \\) is the unit-scaled value, \\( X \\) is the original feature vector, and \\( \\|X\\|_2 \\) is the L2 norm or Euclidean norm of the feature vector.\n",
    "\n",
    "Unit Vector scaling is useful when the direction or orientation of the feature vectors is important, rather than their magnitude. It is commonly used in scenarios where cosine similarity or dot product calculations are employed, such as in recommendation systems or text mining.\n",
    "\n",
    "Here's an example to illustrate the application of the Unit Vector technique:\n",
    "\n",
    "Consider a dataset with two features, \"Height\" and \"Weight,\" represented by a feature vector:\n",
    "\\[ X = \\begin{bmatrix} 170 \\\\ 70 \\end{bmatrix} \\]\n",
    "\n",
    "To apply the Unit Vector scaling, we calculate the L2 norm or Euclidean norm of the feature vector:\n",
    "\\[ \\|X\\|_2 = \\sqrt{{170^2 + 70^2}} \\approx 183.42 \\]\n",
    "\n",
    "Dividing the original feature vector by its L2 norm gives us the unit-scaled value:\n",
    "\\[ X_{\\text{unit}} = \\begin{bmatrix} \\frac{170}{183.42} \\\\ \\frac{70}{183.42} \\end{bmatrix} \\approx \\begin{bmatrix} 0.925 \\\\ 0.383 \\end{bmatrix} \\]\n",
    "\n",
    "So, the feature vector \\([170, 70]\\) is scaled to approximately \\([0.925, 0.383]\\) using the Unit Vector technique. The resulting vector has a unit length, preserving the direction of the original vector.\n",
    "\n",
    "The Unit Vector technique differs from Min-Max scaling in the way it scales the features. Min-Max scaling rescales the feature values to a specific range (e.g., between 0 and 1) by subtracting the minimum value and dividing by the range. On the other hand, the Unit Vector technique normalizes the feature vectors by dividing them by their magnitude, resulting in unit-length vectors.\n",
    "\n",
    "The Unit Vector technique is commonly applied in machine learning tasks where the direction or orientation of feature vectors is crucial for similarity calculations or when the magnitudes of features are not informative.\n",
    "_________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a07037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original vector: [170  70]\n",
      "Unit-scaled vector: [0.9246781  0.38074981]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original feature vector\n",
    "x = np.array([170, 70])\n",
    "\n",
    "# Calculate the L2 norm\n",
    "norm = np.linalg.norm(x)\n",
    "\n",
    "# Unit Vector scaling\n",
    "x_unit = x / norm\n",
    "\n",
    "# Print the original and unit-scaled vectors\n",
    "print(\"Original vector:\", x)\n",
    "print(\"Unit-scaled vector:\", x_unit)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a5abe0",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "Principle Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. It aims to reduce the dimensionality of a dataset by transforming the original features into a new set of uncorrelated variables called principal components. PCA accomplishes this by finding the directions of maximum variance in the data and projecting the data onto these directions.\n",
    "\n",
    "The main steps involved in PCA are as follows:\n",
    "\n",
    "1. **Standardize the Data:** PCA typically requires the data to be standardized to have zero mean and unit variance. This step ensures that all features are on a similar scale, as PCA is sensitive to the relative variances of the features.\n",
    "\n",
    "2. **Compute the Covariance Matrix:** Calculate the covariance matrix of the standardized data. The covariance matrix captures the relationships between different features and their variances.\n",
    "\n",
    "3. **Compute the Eigenvectors and Eigenvalues:** Perform an eigendecomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "4. **Select the Principal Components:** Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues. These principal components capture the most significant variance in the data.\n",
    "\n",
    "5. **Project the Data:** Project the standardized data onto the selected principal components to obtain a lower-dimensional representation of the original data. This projection retains most of the important information while reducing the dimensionality.\n",
    "\n",
    "PCA is commonly used in dimensionality reduction for various reasons, including:\n",
    "\n",
    "- **Feature Compression:** PCA can compress high-dimensional data into a lower-dimensional representation, reducing storage and computational requirements.\n",
    "\n",
    "- **Noise Reduction:** PCA can help remove noise or irrelevant information by emphasizing the principal components that capture the most significant variance in the data.\n",
    "\n",
    "- **Visualization:** PCA can be used to visualize high-dimensional data in two or three dimensions, enabling easier interpretation and analysis.\n",
    "\n",
    "Here's an example of how to apply PCA for dimensionality reduction using Python and the scikit-learn library:\n",
    "\n",
    "__________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fa45a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance ratio: [0.92461872 0.05306648]\n",
      "\n",
      "Transformed data:\n",
      "Original: [5.1 3.5 1.4 0.2] => PCA: [-2.68412563  0.31939725]\n",
      "Original: [4.9 3.  1.4 0.2] => PCA: [-2.71414169 -0.17700123]\n",
      "Original: [4.7 3.2 1.3 0.2] => PCA: [-2.88899057 -0.14494943]\n",
      "Original: [4.6 3.1 1.5 0.2] => PCA: [-2.74534286 -0.31829898]\n",
      "Original: [5.  3.6 1.4 0.2] => PCA: [-2.72871654  0.32675451]\n",
      "Original: [5.4 3.9 1.7 0.4] => PCA: [-2.28085963  0.74133045]\n",
      "Original: [4.6 3.4 1.4 0.3] => PCA: [-2.82053775 -0.08946138]\n",
      "Original: [5.  3.4 1.5 0.2] => PCA: [-2.62614497  0.16338496]\n",
      "Original: [4.4 2.9 1.4 0.2] => PCA: [-2.88638273 -0.57831175]\n",
      "Original: [4.9 3.1 1.5 0.1] => PCA: [-2.6727558  -0.11377425]\n",
      "Original: [5.4 3.7 1.5 0.2] => PCA: [-2.50694709  0.6450689 ]\n",
      "Original: [4.8 3.4 1.6 0.2] => PCA: [-2.61275523  0.01472994]\n",
      "Original: [4.8 3.  1.4 0.1] => PCA: [-2.78610927 -0.235112  ]\n",
      "Original: [4.3 3.  1.1 0.1] => PCA: [-3.22380374 -0.51139459]\n",
      "Original: [5.8 4.  1.2 0.2] => PCA: [-2.64475039  1.17876464]\n",
      "Original: [5.7 4.4 1.5 0.4] => PCA: [-2.38603903  1.33806233]\n",
      "Original: [5.4 3.9 1.3 0.4] => PCA: [-2.62352788  0.81067951]\n",
      "Original: [5.1 3.5 1.4 0.3] => PCA: [-2.64829671  0.31184914]\n",
      "Original: [5.7 3.8 1.7 0.3] => PCA: [-2.19982032  0.87283904]\n",
      "Original: [5.1 3.8 1.5 0.3] => PCA: [-2.5879864   0.51356031]\n",
      "Original: [5.4 3.4 1.7 0.2] => PCA: [-2.31025622  0.39134594]\n",
      "Original: [5.1 3.7 1.5 0.4] => PCA: [-2.54370523  0.43299606]\n",
      "Original: [4.6 3.6 1.  0.2] => PCA: [-3.21593942  0.13346807]\n",
      "Original: [5.1 3.3 1.7 0.5] => PCA: [-2.30273318  0.09870885]\n",
      "Original: [4.8 3.4 1.9 0.2] => PCA: [-2.35575405 -0.03728186]\n",
      "Original: [5.  3.  1.6 0.2] => PCA: [-2.50666891 -0.14601688]\n",
      "Original: [5.  3.4 1.6 0.4] => PCA: [-2.46882007  0.13095149]\n",
      "Original: [5.2 3.5 1.5 0.2] => PCA: [-2.56231991  0.36771886]\n",
      "Original: [5.2 3.4 1.4 0.2] => PCA: [-2.63953472  0.31203998]\n",
      "Original: [4.7 3.2 1.6 0.2] => PCA: [-2.63198939 -0.19696122]\n",
      "Original: [4.8 3.1 1.6 0.2] => PCA: [-2.58739848 -0.20431849]\n",
      "Original: [5.4 3.4 1.5 0.4] => PCA: [-2.4099325   0.41092426]\n",
      "Original: [5.2 4.1 1.5 0.1] => PCA: [-2.64886233  0.81336382]\n",
      "Original: [5.5 4.2 1.4 0.2] => PCA: [-2.59873675  1.09314576]\n",
      "Original: [4.9 3.1 1.5 0.2] => PCA: [-2.63692688 -0.12132235]\n",
      "Original: [5.  3.2 1.2 0.2] => PCA: [-2.86624165  0.06936447]\n",
      "Original: [5.5 3.5 1.3 0.2] => PCA: [-2.62523805  0.59937002]\n",
      "Original: [4.9 3.6 1.4 0.1] => PCA: [-2.80068412  0.26864374]\n",
      "Original: [4.4 3.  1.3 0.2] => PCA: [-2.98050204 -0.48795834]\n",
      "Original: [5.1 3.4 1.5 0.2] => PCA: [-2.59000631  0.22904384]\n",
      "Original: [5.  3.5 1.3 0.3] => PCA: [-2.77010243  0.26352753]\n",
      "Original: [4.5 2.3 1.3 0.3] => PCA: [-2.84936871 -0.94096057]\n",
      "Original: [4.4 3.2 1.3 0.2] => PCA: [-2.99740655 -0.34192606]\n",
      "Original: [5.  3.5 1.6 0.6] => PCA: [-2.40561449  0.18887143]\n",
      "Original: [5.1 3.8 1.9 0.4] => PCA: [-2.20948924  0.43666314]\n",
      "Original: [4.8 3.  1.4 0.3] => PCA: [-2.71445143 -0.2502082 ]\n",
      "Original: [5.1 3.8 1.6 0.2] => PCA: [-2.53814826  0.50377114]\n",
      "Original: [4.6 3.2 1.4 0.2] => PCA: [-2.83946217 -0.22794557]\n",
      "Original: [5.3 3.7 1.5 0.2] => PCA: [-2.54308575  0.57941002]\n",
      "Original: [5.  3.3 1.4 0.2] => PCA: [-2.70335978  0.10770608]\n",
      "Original: [7.  3.2 4.7 1.4] => PCA: [1.28482569 0.68516047]\n",
      "Original: [6.4 3.2 4.5 1.5] => PCA: [0.93248853 0.31833364]\n",
      "Original: [6.9 3.1 4.9 1.5] => PCA: [1.46430232 0.50426282]\n",
      "Original: [5.5 2.3 4.  1.3] => PCA: [ 0.18331772 -0.82795901]\n",
      "Original: [6.5 2.8 4.6 1.5] => PCA: [1.08810326 0.07459068]\n",
      "Original: [5.7 2.8 4.5 1.3] => PCA: [ 0.64166908 -0.41824687]\n",
      "Original: [6.3 3.3 4.7 1.6] => PCA: [1.09506066 0.28346827]\n",
      "Original: [4.9 2.4 3.3 1. ] => PCA: [-0.74912267 -1.00489096]\n",
      "Original: [6.6 2.9 4.6 1.3] => PCA: [1.04413183 0.2283619 ]\n",
      "Original: [5.2 2.7 3.9 1.4] => PCA: [-0.0087454  -0.72308191]\n",
      "Original: [5.  2.  3.5 1. ] => PCA: [-0.50784088 -1.26597119]\n",
      "Original: [5.9 3.  4.2 1.5] => PCA: [ 0.51169856 -0.10398124]\n",
      "Original: [6.  2.2 4.  1. ] => PCA: [ 0.26497651 -0.55003646]\n",
      "Original: [6.1 2.9 4.7 1.4] => PCA: [ 0.98493451 -0.12481785]\n",
      "Original: [5.6 2.9 3.6 1.3] => PCA: [-0.17392537 -0.25485421]\n",
      "Original: [6.7 3.1 4.4 1.4] => PCA: [0.92786078 0.46717949]\n",
      "Original: [5.6 3.  4.5 1.5] => PCA: [ 0.66028376 -0.35296967]\n",
      "Original: [5.8 2.7 4.1 1. ] => PCA: [ 0.23610499 -0.33361077]\n",
      "Original: [6.2 2.2 4.5 1.5] => PCA: [ 0.94473373 -0.54314555]\n",
      "Original: [5.6 2.5 3.9 1.1] => PCA: [ 0.04522698 -0.58383438]\n",
      "Original: [5.9 3.2 4.8 1.8] => PCA: [ 1.11628318 -0.08461685]\n",
      "Original: [6.1 2.8 4.  1.3] => PCA: [ 0.35788842 -0.06892503]\n",
      "Original: [6.3 2.5 4.9 1.5] => PCA: [ 1.29818388 -0.32778731]\n",
      "Original: [6.1 2.8 4.7 1.2] => PCA: [ 0.92172892 -0.18273779]\n",
      "Original: [6.4 2.9 4.3 1.3] => PCA: [0.71485333 0.14905594]\n",
      "Original: [6.6 3.  4.4 1.4] => PCA: [0.90017437 0.32850447]\n",
      "Original: [6.8 2.8 4.8 1.4] => PCA: [1.33202444 0.24444088]\n",
      "Original: [6.7 3.  5.  1.7] => PCA: [1.55780216 0.26749545]\n",
      "Original: [6.  2.9 4.5 1.5] => PCA: [ 0.81329065 -0.1633503 ]\n",
      "Original: [5.7 2.6 3.5 1. ] => PCA: [-0.30558378 -0.36826219]\n",
      "Original: [5.5 2.4 3.8 1.1] => PCA: [-0.06812649 -0.70517213]\n",
      "Original: [5.5 2.4 3.7 1. ] => PCA: [-0.18962247 -0.68028676]\n",
      "Original: [5.8 2.7 3.9 1.2] => PCA: [ 0.13642871 -0.31403244]\n",
      "Original: [6.  2.7 5.1 1.6] => PCA: [ 1.38002644 -0.42095429]\n",
      "Original: [5.4 3.  4.5 1.5] => PCA: [ 0.58800644 -0.48428742]\n",
      "Original: [6.  3.4 4.5 1.6] => PCA: [0.80685831 0.19418231]\n",
      "Original: [6.7 3.1 4.7 1.5] => PCA: [1.22069088 0.40761959]\n",
      "Original: [6.3 2.3 4.4 1.3] => PCA: [ 0.81509524 -0.37203706]\n",
      "Original: [5.6 3.  4.1 1.3] => PCA: [ 0.24595768 -0.2685244 ]\n",
      "Original: [5.5 2.5 4.  1.3] => PCA: [ 0.16641322 -0.68192672]\n",
      "Original: [5.5 2.6 4.4 1.2] => PCA: [ 0.46480029 -0.67071154]\n",
      "Original: [6.1 3.  4.6 1.4] => PCA: [ 0.8908152  -0.03446444]\n",
      "Original: [5.8 2.6 4.  1.2] => PCA: [ 0.23054802 -0.40438585]\n",
      "Original: [5.  2.3 3.3 1. ] => PCA: [-0.70453176 -1.01224823]\n",
      "Original: [5.6 2.7 4.2 1.3] => PCA: [ 0.35698149 -0.50491009]\n",
      "Original: [5.7 3.  4.2 1.2] => PCA: [ 0.33193448 -0.21265468]\n",
      "Original: [5.7 2.9 4.2 1.3] => PCA: [ 0.37621565 -0.29321893]\n",
      "Original: [6.2 2.9 4.3 1.3] => PCA: [0.64257601 0.01773819]\n",
      "Original: [5.1 2.5 3.  1.1] => PCA: [-0.90646986 -0.75609337]\n",
      "Original: [5.7 2.8 4.1 1.3] => PCA: [ 0.29900084 -0.34889781]\n",
      "Original: [6.3 3.3 6.  2.5] => PCA: [ 2.53119273 -0.00984911]\n",
      "Original: [5.8 2.7 5.1 1.9] => PCA: [ 1.41523588 -0.57491635]\n",
      "Original: [7.1 3.  5.9 2.1] => PCA: [2.61667602 0.34390315]\n",
      "Original: [6.3 2.9 5.6 1.8] => PCA: [ 1.97153105 -0.1797279 ]\n",
      "Original: [6.5 3.  5.8 2.2] => PCA: [ 2.35000592 -0.04026095]\n",
      "Original: [7.6 3.  6.6 2.1] => PCA: [3.39703874 0.55083667]\n",
      "Original: [4.9 2.5 4.5 1.7] => PCA: [ 0.52123224 -1.19275873]\n",
      "Original: [7.3 2.9 6.3 1.8] => PCA: [2.93258707 0.3555    ]\n",
      "Original: [6.7 2.5 5.8 1.8] => PCA: [ 2.32122882 -0.2438315 ]\n",
      "Original: [7.2 3.6 6.1 2.5] => PCA: [2.91675097 0.78279195]\n",
      "Original: [6.5 3.2 5.1 2. ] => PCA: [1.66177415 0.24222841]\n",
      "Original: [6.4 2.7 5.3 1.9] => PCA: [ 1.80340195 -0.21563762]\n",
      "Original: [6.8 3.  5.5 2.1] => PCA: [2.1655918  0.21627559]\n",
      "Original: [5.7 2.5 5.  2. ] => PCA: [ 1.34616358 -0.77681835]\n",
      "Original: [5.8 2.8 5.1 2.4] => PCA: [ 1.58592822 -0.53964071]\n",
      "Original: [6.4 3.2 5.3 2.3] => PCA: [1.90445637 0.11925069]\n",
      "Original: [6.5 3.  5.5 1.8] => PCA: [1.94968906 0.04194326]\n",
      "Original: [7.7 3.8 6.7 2.2] => PCA: [3.48705536 1.17573933]\n",
      "Original: [7.7 2.6 6.9 2.3] => PCA: [3.79564542 0.25732297]\n",
      "Original: [6.  2.2 5.  1.5] => PCA: [ 1.30079171 -0.76114964]\n",
      "Original: [6.9 3.2 5.7 2.3] => PCA: [2.42781791 0.37819601]\n",
      "Original: [5.6 2.8 4.9 2. ] => PCA: [ 1.19900111 -0.60609153]\n",
      "Original: [7.7 2.8 6.7 2. ] => PCA: [3.49992004 0.4606741 ]\n",
      "Original: [6.3 2.7 4.9 1.8] => PCA: [ 1.38876613 -0.20439933]\n",
      "Original: [6.7 3.3 5.7 2.1] => PCA: [2.2754305  0.33499061]\n",
      "Original: [7.2 3.2 6.  1.8] => PCA: [2.61409047 0.56090136]\n",
      "Original: [6.2 2.8 4.8 1.8] => PCA: [ 1.25850816 -0.17970479]\n",
      "Original: [6.1 3.  4.9 1.8] => PCA: [ 1.29113206 -0.11666865]\n",
      "Original: [6.4 2.8 5.6 2.1] => PCA: [ 2.12360872 -0.20972948]\n",
      "Original: [7.2 3.  5.8 1.6] => PCA: [2.38800302 0.4646398 ]\n",
      "Original: [7.4 2.8 6.1 1.9] => PCA: [2.84167278 0.37526917]\n",
      "Original: [7.9 3.8 6.4 2. ] => PCA: [3.23067366 1.37416509]\n",
      "Original: [6.4 2.8 5.6 2.2] => PCA: [ 2.15943764 -0.21727758]\n",
      "Original: [6.3 2.8 5.1 1.5] => PCA: [ 1.44416124 -0.14341341]\n",
      "Original: [6.1 2.6 5.6 1.4] => PCA: [ 1.78129481 -0.49990168]\n",
      "Original: [7.7 3.  6.1 2.3] => PCA: [3.07649993 0.68808568]\n",
      "Original: [6.3 3.4 5.6 2.4] => PCA: [2.14424331 0.1400642 ]\n",
      "Original: [6.4 3.1 5.5 1.8] => PCA: [1.90509815 0.04930053]\n",
      "Original: [6.  3.  4.8 1.8] => PCA: [ 1.16932634 -0.16499026]\n",
      "Original: [6.9 3.1 5.4 2.1] => PCA: [2.10761114 0.37228787]\n",
      "Original: [6.7 3.1 5.6 2.4] => PCA: [2.31415471 0.18365128]\n",
      "Original: [6.9 3.1 5.1 2.3] => PCA: [1.9222678  0.40920347]\n",
      "Original: [5.8 2.7 5.1 1.9] => PCA: [ 1.41523588 -0.57491635]\n",
      "Original: [6.8 3.2 5.9 2.3] => PCA: [2.56301338 0.2778626 ]\n",
      "Original: [6.7 3.3 5.7 2.5] => PCA: [2.41874618 0.3047982 ]\n",
      "Original: [6.7 3.  5.2 2.3] => PCA: [1.94410979 0.1875323 ]\n",
      "Original: [6.3 2.5 5.  1.9] => PCA: [ 1.52716661 -0.37531698]\n",
      "Original: [6.5 3.  5.2 2. ] => PCA: [1.76434572 0.07885885]\n",
      "Original: [6.2 3.4 5.4 2.3] => PCA: [1.90094161 0.11662796]\n",
      "Original: [5.9 3.  5.1 1.8] => PCA: [ 1.39018886 -0.28266094]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Create a PCA object with 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"\\nTransformed data:\")\n",
    "for i, (x, xpca) in enumerate(zip(X, X_pca)):\n",
    "    print(\"Original:\", x, \"=> PCA:\", xpca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4224f2d4",
   "metadata": {},
   "source": [
    "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "PCA and feature extraction are closely related concepts. PCA can be used as a feature extraction technique to transform high-dimensional data into a lower-dimensional space by selecting the most informative features.\n",
    "\n",
    "In the context of PCA, feature extraction refers to the process of creating new features, known as principal components, that capture the most significant variation in the original dataset. These principal components are linear combinations of the original features and are orthogonal (i.e., uncorrelated) to each other.\n",
    "\n",
    "PCA for feature extraction involves the following steps:\n",
    "\n",
    "Standardize the Data: It is recommended to standardize the data by subtracting the mean and dividing by the standard deviation to ensure that all features are on a similar scale.\n",
    "\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix captures the relationships between different features and their variances.\n",
    "\n",
    "Compute the Eigenvectors and Eigenvalues: Perform an eigendecomposition on the covariance matrix to obtain the eigenvectors and eigenvalues. The eigenvectors represent the principal components, and the corresponding eigenvalues represent the amount of variance explained by each principal component.\n",
    "\n",
    "Select the Principal Components: Sort the eigenvalues in descending order and select the top k eigenvectors corresponding to the largest eigenvalues. These principal components capture the most significant variance in the data.\n",
    "\n",
    "Transform the Data: Project the standardized data onto the selected principal components to obtain a lower-dimensional representation of the original data. This transformed data represents the extracted features.\n",
    "\n",
    "The main difference between PCA for feature extraction and traditional dimensionality reduction using PCA is that in feature extraction, the focus is on creating new features that capture the most important information in the data, rather than reducing the dimensionality for the purpose of compression or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b70f3b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed features:\n",
      "Sample 1 : [-2.68412563  0.31939725]\n",
      "Sample 2 : [-2.71414169 -0.17700123]\n",
      "Sample 3 : [-2.88899057 -0.14494943]\n",
      "Sample 4 : [-2.74534286 -0.31829898]\n",
      "Sample 5 : [-2.72871654  0.32675451]\n",
      "Sample 6 : [-2.28085963  0.74133045]\n",
      "Sample 7 : [-2.82053775 -0.08946138]\n",
      "Sample 8 : [-2.62614497  0.16338496]\n",
      "Sample 9 : [-2.88638273 -0.57831175]\n",
      "Sample 10 : [-2.6727558  -0.11377425]\n",
      "Sample 11 : [-2.50694709  0.6450689 ]\n",
      "Sample 12 : [-2.61275523  0.01472994]\n",
      "Sample 13 : [-2.78610927 -0.235112  ]\n",
      "Sample 14 : [-3.22380374 -0.51139459]\n",
      "Sample 15 : [-2.64475039  1.17876464]\n",
      "Sample 16 : [-2.38603903  1.33806233]\n",
      "Sample 17 : [-2.62352788  0.81067951]\n",
      "Sample 18 : [-2.64829671  0.31184914]\n",
      "Sample 19 : [-2.19982032  0.87283904]\n",
      "Sample 20 : [-2.5879864   0.51356031]\n",
      "Sample 21 : [-2.31025622  0.39134594]\n",
      "Sample 22 : [-2.54370523  0.43299606]\n",
      "Sample 23 : [-3.21593942  0.13346807]\n",
      "Sample 24 : [-2.30273318  0.09870885]\n",
      "Sample 25 : [-2.35575405 -0.03728186]\n",
      "Sample 26 : [-2.50666891 -0.14601688]\n",
      "Sample 27 : [-2.46882007  0.13095149]\n",
      "Sample 28 : [-2.56231991  0.36771886]\n",
      "Sample 29 : [-2.63953472  0.31203998]\n",
      "Sample 30 : [-2.63198939 -0.19696122]\n",
      "Sample 31 : [-2.58739848 -0.20431849]\n",
      "Sample 32 : [-2.4099325   0.41092426]\n",
      "Sample 33 : [-2.64886233  0.81336382]\n",
      "Sample 34 : [-2.59873675  1.09314576]\n",
      "Sample 35 : [-2.63692688 -0.12132235]\n",
      "Sample 36 : [-2.86624165  0.06936447]\n",
      "Sample 37 : [-2.62523805  0.59937002]\n",
      "Sample 38 : [-2.80068412  0.26864374]\n",
      "Sample 39 : [-2.98050204 -0.48795834]\n",
      "Sample 40 : [-2.59000631  0.22904384]\n",
      "Sample 41 : [-2.77010243  0.26352753]\n",
      "Sample 42 : [-2.84936871 -0.94096057]\n",
      "Sample 43 : [-2.99740655 -0.34192606]\n",
      "Sample 44 : [-2.40561449  0.18887143]\n",
      "Sample 45 : [-2.20948924  0.43666314]\n",
      "Sample 46 : [-2.71445143 -0.2502082 ]\n",
      "Sample 47 : [-2.53814826  0.50377114]\n",
      "Sample 48 : [-2.83946217 -0.22794557]\n",
      "Sample 49 : [-2.54308575  0.57941002]\n",
      "Sample 50 : [-2.70335978  0.10770608]\n",
      "Sample 51 : [1.28482569 0.68516047]\n",
      "Sample 52 : [0.93248853 0.31833364]\n",
      "Sample 53 : [1.46430232 0.50426282]\n",
      "Sample 54 : [ 0.18331772 -0.82795901]\n",
      "Sample 55 : [1.08810326 0.07459068]\n",
      "Sample 56 : [ 0.64166908 -0.41824687]\n",
      "Sample 57 : [1.09506066 0.28346827]\n",
      "Sample 58 : [-0.74912267 -1.00489096]\n",
      "Sample 59 : [1.04413183 0.2283619 ]\n",
      "Sample 60 : [-0.0087454  -0.72308191]\n",
      "Sample 61 : [-0.50784088 -1.26597119]\n",
      "Sample 62 : [ 0.51169856 -0.10398124]\n",
      "Sample 63 : [ 0.26497651 -0.55003646]\n",
      "Sample 64 : [ 0.98493451 -0.12481785]\n",
      "Sample 65 : [-0.17392537 -0.25485421]\n",
      "Sample 66 : [0.92786078 0.46717949]\n",
      "Sample 67 : [ 0.66028376 -0.35296967]\n",
      "Sample 68 : [ 0.23610499 -0.33361077]\n",
      "Sample 69 : [ 0.94473373 -0.54314555]\n",
      "Sample 70 : [ 0.04522698 -0.58383438]\n",
      "Sample 71 : [ 1.11628318 -0.08461685]\n",
      "Sample 72 : [ 0.35788842 -0.06892503]\n",
      "Sample 73 : [ 1.29818388 -0.32778731]\n",
      "Sample 74 : [ 0.92172892 -0.18273779]\n",
      "Sample 75 : [0.71485333 0.14905594]\n",
      "Sample 76 : [0.90017437 0.32850447]\n",
      "Sample 77 : [1.33202444 0.24444088]\n",
      "Sample 78 : [1.55780216 0.26749545]\n",
      "Sample 79 : [ 0.81329065 -0.1633503 ]\n",
      "Sample 80 : [-0.30558378 -0.36826219]\n",
      "Sample 81 : [-0.06812649 -0.70517213]\n",
      "Sample 82 : [-0.18962247 -0.68028676]\n",
      "Sample 83 : [ 0.13642871 -0.31403244]\n",
      "Sample 84 : [ 1.38002644 -0.42095429]\n",
      "Sample 85 : [ 0.58800644 -0.48428742]\n",
      "Sample 86 : [0.80685831 0.19418231]\n",
      "Sample 87 : [1.22069088 0.40761959]\n",
      "Sample 88 : [ 0.81509524 -0.37203706]\n",
      "Sample 89 : [ 0.24595768 -0.2685244 ]\n",
      "Sample 90 : [ 0.16641322 -0.68192672]\n",
      "Sample 91 : [ 0.46480029 -0.67071154]\n",
      "Sample 92 : [ 0.8908152  -0.03446444]\n",
      "Sample 93 : [ 0.23054802 -0.40438585]\n",
      "Sample 94 : [-0.70453176 -1.01224823]\n",
      "Sample 95 : [ 0.35698149 -0.50491009]\n",
      "Sample 96 : [ 0.33193448 -0.21265468]\n",
      "Sample 97 : [ 0.37621565 -0.29321893]\n",
      "Sample 98 : [0.64257601 0.01773819]\n",
      "Sample 99 : [-0.90646986 -0.75609337]\n",
      "Sample 100 : [ 0.29900084 -0.34889781]\n",
      "Sample 101 : [ 2.53119273 -0.00984911]\n",
      "Sample 102 : [ 1.41523588 -0.57491635]\n",
      "Sample 103 : [2.61667602 0.34390315]\n",
      "Sample 104 : [ 1.97153105 -0.1797279 ]\n",
      "Sample 105 : [ 2.35000592 -0.04026095]\n",
      "Sample 106 : [3.39703874 0.55083667]\n",
      "Sample 107 : [ 0.52123224 -1.19275873]\n",
      "Sample 108 : [2.93258707 0.3555    ]\n",
      "Sample 109 : [ 2.32122882 -0.2438315 ]\n",
      "Sample 110 : [2.91675097 0.78279195]\n",
      "Sample 111 : [1.66177415 0.24222841]\n",
      "Sample 112 : [ 1.80340195 -0.21563762]\n",
      "Sample 113 : [2.1655918  0.21627559]\n",
      "Sample 114 : [ 1.34616358 -0.77681835]\n",
      "Sample 115 : [ 1.58592822 -0.53964071]\n",
      "Sample 116 : [1.90445637 0.11925069]\n",
      "Sample 117 : [1.94968906 0.04194326]\n",
      "Sample 118 : [3.48705536 1.17573933]\n",
      "Sample 119 : [3.79564542 0.25732297]\n",
      "Sample 120 : [ 1.30079171 -0.76114964]\n",
      "Sample 121 : [2.42781791 0.37819601]\n",
      "Sample 122 : [ 1.19900111 -0.60609153]\n",
      "Sample 123 : [3.49992004 0.4606741 ]\n",
      "Sample 124 : [ 1.38876613 -0.20439933]\n",
      "Sample 125 : [2.2754305  0.33499061]\n",
      "Sample 126 : [2.61409047 0.56090136]\n",
      "Sample 127 : [ 1.25850816 -0.17970479]\n",
      "Sample 128 : [ 1.29113206 -0.11666865]\n",
      "Sample 129 : [ 2.12360872 -0.20972948]\n",
      "Sample 130 : [2.38800302 0.4646398 ]\n",
      "Sample 131 : [2.84167278 0.37526917]\n",
      "Sample 132 : [3.23067366 1.37416509]\n",
      "Sample 133 : [ 2.15943764 -0.21727758]\n",
      "Sample 134 : [ 1.44416124 -0.14341341]\n",
      "Sample 135 : [ 1.78129481 -0.49990168]\n",
      "Sample 136 : [3.07649993 0.68808568]\n",
      "Sample 137 : [2.14424331 0.1400642 ]\n",
      "Sample 138 : [1.90509815 0.04930053]\n",
      "Sample 139 : [ 1.16932634 -0.16499026]\n",
      "Sample 140 : [2.10761114 0.37228787]\n",
      "Sample 141 : [2.31415471 0.18365128]\n",
      "Sample 142 : [1.9222678  0.40920347]\n",
      "Sample 143 : [ 1.41523588 -0.57491635]\n",
      "Sample 144 : [2.56301338 0.2778626 ]\n",
      "Sample 145 : [2.41874618 0.3047982 ]\n",
      "Sample 146 : [1.94410979 0.1875323 ]\n",
      "Sample 147 : [ 1.52716661 -0.37531698]\n",
      "Sample 148 : [1.76434572 0.07885885]\n",
      "Sample 149 : [1.90094161 0.11662796]\n",
      "Sample 150 : [ 1.39018886 -0.28266094]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Create a PCA object with 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA model to the data and extract features\n",
    "X_features = pca.fit_transform(X)\n",
    "\n",
    "# Print the transformed features\n",
    "print(\"Transformed features:\")\n",
    "for i, x in enumerate(X_features):\n",
    "    print(\"Sample\", i+1, \":\", x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dbffa9",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1465602b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Load the dataset into a pandas DataFrame\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Separate the features from the target variable (if applicable)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m features \u001b[38;5;241m=\u001b[39m data[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelivery_time\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
     ]
    }
   ],
   "source": [
    "# To preprocess the data for a recommendation system in Python using Min-Max scaling, you can follow these steps:\n",
    "#Import the necessary libraries:\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset into a pandas DataFrame\n",
    "data = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Separate the features from the target variable (if applicable)\n",
    "features = data[['price', 'rating', 'delivery_time']]\n",
    "\n",
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the features using the scaler\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "# Create a new DataFrame with the scaled features\n",
    "scaled_data = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "# Concatenate the scaled features with the target variable (if applicable)\n",
    "# scaled_data = pd.concat([scaled_features, data['target_variable']], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb3f0c5",
   "metadata": {},
   "source": [
    "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07cf4e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset:\n",
      "   Feature_1  Feature_2  Feature_3  Feature_4  Feature_5  Feature_6  \\\n",
      "0   0.496714  -0.138264   0.647689   1.523030  -0.234153  -0.234137   \n",
      "1  -0.463418  -0.465730   0.241962  -1.913280  -1.724918  -0.562288   \n",
      "2   1.465649  -0.225776   0.067528  -1.424748  -0.544383   0.110923   \n",
      "3  -0.601707   1.852278  -0.013497  -1.057711   0.822545  -1.220844   \n",
      "4   0.738467   0.171368  -0.115648  -0.301104  -1.478522  -0.719844   \n",
      "\n",
      "   Feature_7  Feature_8  Feature_9  Feature_10  \n",
      "0   1.579213   0.767435  -0.469474    0.542560  \n",
      "1  -1.012831   0.314247  -0.908024   -1.412304  \n",
      "2  -1.150994   0.375698  -0.600639   -0.291694  \n",
      "3   0.208864  -1.959670  -1.328186    0.196861  \n",
      "4  -0.460639   1.057122   0.343618   -1.763040  \n",
      "Reduced Dataset:\n",
      "        PC1       PC2\n",
      "0 -0.707326  0.431615\n",
      "1 -0.327949 -0.788912\n",
      "2  0.243385 -0.622397\n",
      "3 -1.832261  0.496534\n",
      "4 -0.024826  0.699927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n_samples = 1000  # Number of samples\n",
    "n_features = 10  # Number of features\n",
    "\n",
    "# Generate random values for the features\n",
    "data = np.random.randn(n_samples, n_features)\n",
    "\n",
    "# Create a DataFrame from the synthetic dataset\n",
    "df = pd.DataFrame(data, columns=[f\"Feature_{i+1}\" for i in range(n_features)])\n",
    "\n",
    "# Print the original dataset\n",
    "print(\"Original Dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)  # Specify the number of principal components to retain\n",
    "reduced_features = pca.fit_transform(df)\n",
    "\n",
    "# Create a new DataFrame with the reduced features\n",
    "reduced_df = pd.DataFrame(reduced_features, columns=[\"PC1\", \"PC2\"])\n",
    "\n",
    "# Print the reduced dataset\n",
    "print(\"Reduced Dataset:\")\n",
    "print(reduced_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9ddbc24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min-Max scaled values: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "    #values to a range of -1 to 1.\n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "dataset = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the new range\n",
    "new_min = -1\n",
    "new_max = 1\n",
    "\n",
    "# Calculate the Min-Max scaled values\n",
    "scaled_values = ((dataset - np.min(dataset)) / (np.max(dataset) - np.min(dataset))) * (new_max - new_min) + new_min\n",
    "\n",
    "# Print the scaled values\n",
    "print(\"Min-Max scaled values:\", scaled_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8fa1b4",
   "metadata": {},
   "source": [
    "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b0334a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
